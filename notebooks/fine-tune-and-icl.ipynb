{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/shilu10/Few-Shot-Learning-Pre-trained_-LM.git/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T14:06:18.211622Z","iopub.execute_input":"2023-11-20T14:06:18.212732Z","iopub.status.idle":"2023-11-20T14:06:19.751595Z","shell.execute_reply.started":"2023-11-20T14:06:18.212694Z","shell.execute_reply":"2023-11-20T14:06:19.750614Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Cloning into 'Few-Shot-Learning-Pre-trained_-LM'...\nremote: Enumerating objects: 141, done.\u001b[K\nremote: Counting objects: 100% (141/141), done.\u001b[K\nremote: Compressing objects: 100% (63/63), done.\u001b[K\nremote: Total 141 (delta 37), reused 121 (delta 21), pack-reused 0\u001b[K\nReceiving objects: 100% (141/141), 581.29 KiB | 13.21 MiB/s, done.\nResolving deltas: 100% (37/37), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp -r /kaggle/working/Few-Shot-Learning-Pre-trained_-LM/* .","metadata":{"execution":{"iopub.status.busy":"2023-11-20T14:06:22.092488Z","iopub.execute_input":"2023-11-20T14:06:22.092850Z","iopub.status.idle":"2023-11-20T14:06:23.065238Z","shell.execute_reply.started":"2023-11-20T14:06:22.092822Z","shell.execute_reply":"2023-11-20T14:06:23.064007Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/working/Few-Shot-Learning-Pre-trained_-LM/starter_code/* .","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:50:24.549892Z","iopub.execute_input":"2023-11-18T14:50:24.550274Z","iopub.status.idle":"2023-11-18T14:50:25.477646Z","shell.execute_reply.started":"2023-11-18T14:50:24.550234Z","shell.execute_reply":"2023-11-18T14:50:25.476580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r requirements.txt --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning BERT MED, FULL models with yelp dataset (with few-shot learning setup)","metadata":{}},{"cell_type":"code","source":"# ft bert\n!python3 ft.py --task ft  --model bert-tiny,bert-med --dataset yelp --k 1,8,128","metadata":{"execution":{"iopub.status.busy":"2023-11-20T14:08:10.142717Z","iopub.execute_input":"2023-11-20T14:08:10.143475Z","iopub.status.idle":"2023-11-20T14:08:52.719725Z","shell.execute_reply.started":"2023-11-20T14:08:10.143440Z","shell.execute_reply":"2023-11-20T14:08:52.718733Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Using results dir: results\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 470.50it/s]\nSome weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nFine-tuning bert-tiny on yelp with k=1 and mode=all\nFine-tuning acc: 1.0000:   1%|▏               | 10/1000 [00:00<00:31, 31.77it/s]\n{'bert-tiny_yelp_1_all': 0.19679999351501465}\nFine-tuning bert-tiny on yelp with k=8 and mode=all\nFine-tuning acc: 0.7750:   3%|▍               | 30/1000 [00:00<00:12, 78.40it/s]\n{'bert-tiny_yelp_8_all': 0.2639999985694885}\nFine-tuning bert-tiny on yelp with k=128 and mode=all\nFine-tuning acc: 0.7781:  32%|████▊          | 320/1000 [00:04<00:09, 69.82it/s]\n{'bert-tiny_yelp_128_all': 0.37119999527931213}\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nFine-tuning bert-med on yelp with k=1 and mode=all\nFine-tuning acc: 1.0000:   1%|▏               | 10/1000 [00:00<00:41, 23.71it/s]\n{'bert-med_yelp_1_all': 0.2272000014781952}\nFine-tuning bert-med on yelp with k=8 and mode=all\nFine-tuning acc: 0.8500:   2%|▎               | 20/1000 [00:00<00:46, 21.28it/s]\n{'bert-med_yelp_8_all': 0.25599998235702515}\nFine-tuning bert-med on yelp with k=128 and mode=all\nFine-tuning acc: 0.8031:  18%|██▋            | 180/1000 [00:16<01:16, 10.71it/s]\n{'bert-med_yelp_128_all': 0.3391999900341034}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## In-Context Learning with GPT MED, FULL models with Babi dataset with multiple K-shots","metadata":{}},{"cell_type":"code","source":"!python icl.py --task icl --model med,full --dataset babi --k 0,1,16","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:50:26.527229Z","iopub.execute_input":"2023-11-18T14:50:26.528176Z","iopub.status.idle":"2023-11-18T14:54:30.015434Z","shell.execute_reply.started":"2023-11-18T14:50:26.528139Z","shell.execute_reply":"2023-11-18T14:54:30.014268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python icl.py --task plot --model med,full --dataset babi --k 0,1,16 --plot_name=\"Q1_2.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:56:03.806975Z","iopub.execute_input":"2023-11-18T14:56:03.807378Z","iopub.status.idle":"2023-11-18T14:56:08.573427Z","shell.execute_reply.started":"2023-11-18T14:56:03.807340Z","shell.execute_reply":"2023-11-18T14:56:08.572441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In-Context Learning with GPT MED, FULL models with xsum dataset with multiple K-shots and different prompt template","metadata":{}},{"cell_type":"code","source":"!python icl.py --task icl --model med,full --dataset xsum --k 0,1,4 --prompt none,tldr,custom","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:57:18.627605Z","iopub.execute_input":"2023-11-18T14:57:18.628802Z","iopub.status.idle":"2023-11-18T16:26:01.361195Z","shell.execute_reply.started":"2023-11-18T14:57:18.628748Z","shell.execute_reply":"2023-11-18T16:26:01.360111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python icl.py --task plot --model med,full --dataset xsum --k 0,1,4  --prompt none,tldr,custom --plot name=\"Q1_3.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-18T16:26:10.298536Z","iopub.execute_input":"2023-11-18T16:26:10.299260Z","iopub.status.idle":"2023-11-18T16:26:14.883622Z","shell.execute_reply.started":"2023-11-18T16:26:10.299221Z","shell.execute_reply":"2023-11-18T16:26:14.882452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning GPT MED, FULL with xsum, babi dataset with different modes(like first, last, lora..) with few-shot learning setup","metadata":{}},{"cell_type":"code","source":"!python ft.py --task ft --model med --mode first,last,middle,lora4,lora16 --dataset xsum,babi --k 0,1,8,32","metadata":{"execution":{"iopub.status.busy":"2023-11-18T16:54:35.778058Z","iopub.execute_input":"2023-11-18T16:54:35.778456Z","iopub.status.idle":"2023-11-18T17:25:02.271010Z","shell.execute_reply.started":"2023-11-18T16:54:35.778424Z","shell.execute_reply":"2023-11-18T17:25:02.269822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ft.py --task plot --model med --mode first,last,middle,lora4,lora16 \\\n--dataset xsum --k 0,1,8,32 --plot_name=\"Q2_3_xsum.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:37:01.937301Z","iopub.execute_input":"2023-11-18T17:37:01.938337Z","iopub.status.idle":"2023-11-18T17:37:06.393393Z","shell.execute_reply.started":"2023-11-18T17:37:01.938297Z","shell.execute_reply":"2023-11-18T17:37:06.392347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ft.py --task plot --model med --mode first,last,middle,lora4,lora16 \\\n--dataset babi --k 0,1,8,32 --plot_name=\"Q2_3_babi.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:37:28.697298Z","iopub.execute_input":"2023-11-18T17:37:28.698231Z","iopub.status.idle":"2023-11-18T17:37:33.191521Z","shell.execute_reply.started":"2023-11-18T17:37:28.698190Z","shell.execute_reply":"2023-11-18T17:37:33.190378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python icl.py --task icl --model med --dataset babi --k 16 --repeats 5","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:50:06.617963Z","iopub.execute_input":"2023-11-18T17:50:06.619305Z","iopub.status.idle":"2023-11-18T17:51:36.651812Z","shell.execute_reply.started":"2023-11-18T17:50:06.619270Z","shell.execute_reply":"2023-11-18T17:51:36.650821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python q3_plot.py","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:56:19.257834Z","iopub.execute_input":"2023-11-18T17:56:19.258217Z","iopub.status.idle":"2023-11-18T17:56:22.853889Z","shell.execute_reply.started":"2023-11-18T17:56:19.258184Z","shell.execute_reply":"2023-11-18T17:56:22.852717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r ft_and_icl_new.zip  results/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}